{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the \"Cranial implant prediction by learning an ensemble of slice-based skull completion networks\"\n",
    "please just run this code untill the build model part. At that part you find the next instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import nrrd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3 as sql\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage import transform, measure\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code For Generate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEBlock2(x, sq_rate=4):\n",
    "    inp_x = x\n",
    "    filters = x.shape[-1]\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Reshape((filters, ))(x)\n",
    "    x = layers.Dense(filters//sq_rate)(x)\n",
    "    x = layers.LeakyReLU(.2)(x)\n",
    "    x = layers.Dense(filters, activation='sigmoid')(x)\n",
    "    x = tf.nn.softmax(x)\n",
    "    x = layers.Reshape((1, 1, filters))(x)\n",
    "    x = inp_x * x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEBlock(x, sq_rate=4):\n",
    "    inp_x = x\n",
    "    filters = x.shape[-1]\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Reshape((filters, ))(x)\n",
    "    x = layers.Dense(filters//sq_rate)(x)\n",
    "    x = layers.LeakyReLU(.3)(x)\n",
    "    x = layers.Dense(filters, activation='sigmoid')(x)\n",
    "    x = tf.nn.softmax(x)\n",
    "    x = layers.Reshape((1, 1, filters))(x)\n",
    "    x = inp_x * x * filters\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChannelGainInit(shape, dtype=None):\n",
    "    print(shape, dtype)\n",
    "    return 128*tf.ones(shape, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelGain(layers.Layer):\n",
    "    def __init__(self, ini_val):\n",
    "        super().__init__()\n",
    "        self.ini_val = ini_val\n",
    "    def build(self, input_shape):\n",
    "        self.gain = self.add_weight(\n",
    "            shape=(1, ),\n",
    "            initializer=ChannelGainInit,\n",
    "            trainable=True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.gain\n",
    "    def get_config(self):\n",
    "        return {'ini_val': self.ini_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeakyConv2D(x, f, ks, st=1, pd='same', dr=1):\n",
    "    x = layers.Conv2D(f, ks, strides=st, padding=pd, dilation_rate=dr)(x)\n",
    "    x = layers.LeakyReLU(.2)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeakyConv2DT(x, f, ks, st=1, pd='same', dr=1):\n",
    "    x = layers.Conv2DTranspose(f, ks, strides=st, padding=pd, dilation_rate=dr)(x)\n",
    "    x = layers.LeakyReLU(.2)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionHead(shape, f):\n",
    "    ginp_x = layers.Input(shape)\n",
    "    x = ginp_x\n",
    "    x = LeakyConv2D(x, f, 3, st=2)\n",
    "    x = LeakyConv2D(x, f, 3)\n",
    "    x = LeakyConv2D(x, f*2, 3)\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    out_x.append(x)\n",
    "    # Path_2\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*3, 3, st=2)\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*2, 1)\n",
    "    x = LeakyConv2D(x, f*3, 3)\n",
    "    out_x.append(x)\n",
    "    # Path 2\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*2, 1)\n",
    "    x = LeakyConv2D(x, f*2, (1, 7))\n",
    "    x = LeakyConv2D(x, f*2, (7, 1))\n",
    "    x = LeakyConv2D(x, f*3, 3)\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*6, 3, st=2)\n",
    "    out_x.append(x)\n",
    "    # Path 2\n",
    "    x = inp_x\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    return keras.Model(ginp_x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionA(shape, f):\n",
    "    ginp_x = layers.Input(shape)\n",
    "    x = ginp_x\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    ori_shape = x.shape[1:-1]\n",
    "    ori_filters = x.shape[-1]\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Reshape((1, 1, ori_filters))(x)\n",
    "    x = layers.UpSampling2D(ori_shape)(x)\n",
    "    x = LeakyConv2D(x, f*3, 1)\n",
    "    out_x.append(x)\n",
    "    # Path 2\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*3, 1)\n",
    "    out_x.append(x)\n",
    "    # Path 3\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*2, 1)\n",
    "    x = LeakyConv2D(x, f*3, 3)\n",
    "    out_x.append(x)\n",
    "    # Path 4\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*2, 1)\n",
    "    x = LeakyConv2D(x, f*3, 3)\n",
    "    x = LeakyConv2D(x, f*3, 3)\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    return keras.Model(ginp_x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionB(shape):\n",
    "    ginp_x = layers.Input(shape)\n",
    "    f = shape[-1]//8\n",
    "    x = ginp_x\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    ori_shape = x.shape[1:-1]\n",
    "    ori_filters = x.shape[-1]\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Reshape((1, 1, ori_filters))(x)\n",
    "    x = layers.UpSampling2D(ori_shape)(x)\n",
    "    x = LeakyConv2D(x, f*2, 1)\n",
    "    # out_x.append(x)\n",
    "    # Path 2\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*6, 1)\n",
    "    # out_x.append(x)\n",
    "    # Path 3\n",
    "    x = inp_x\n",
    "    # x = LeakyConv2D(x, f*3, 1)\n",
    "    x = LeakyConv2D(x, f*4, (1, 7))\n",
    "    x = LeakyConv2D(x, f*4, (7, 1))\n",
    "    out_x.append(x)\n",
    "    # Path 4\n",
    "    x = inp_x\n",
    "    # x = LeakyConv2D(x, f*3, 1)\n",
    "    x = LeakyConv2D(x, f*4, (1, 7))\n",
    "    x = LeakyConv2D(x, f*4, (7, 1))\n",
    "    x = LeakyConv2D(x, f*4, (1, 7))\n",
    "    x = LeakyConv2D(x, f*4, (7, 1))\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    return keras.Model(ginp_x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionC(shape, f):\n",
    "    ginp_x = layers.Input(shape)\n",
    "    x = ginp_x\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    ori_shape = x.shape[1:-1]\n",
    "    ori_filters = x.shape[-1]\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Reshape((1, 1, ori_filters))(x)\n",
    "    x = layers.UpSampling2D(ori_shape)(x)\n",
    "    x = LeakyConv2D(x, f*4, 1)\n",
    "    out_x.append(x)\n",
    "    # Path 2\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*4, 1)\n",
    "    out_x.append(x)\n",
    "    # Path 3\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*6, 1)\n",
    "    inp2_x = x\n",
    "    # Path L\n",
    "    x = inp2_x\n",
    "    x = LeakyConv2D(x, f*4, (1, 3))\n",
    "    out_x.append(x)\n",
    "    # Path R\n",
    "    x = inp2_x\n",
    "    x = LeakyConv2D(x, f*4, (3, 1))\n",
    "    out_x.append(x)\n",
    "    # Path 4\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*6, 1)\n",
    "    x = LeakyConv2D(x, f*8, (1, 3))\n",
    "    x = LeakyConv2D(x, f*8, (3, 1))\n",
    "    inp2_x = x\n",
    "    # Path L\n",
    "    x = inp2_x\n",
    "    x = LeakyConv2D(x, f*4, (1, 3))\n",
    "    out_x.append(x)\n",
    "    # Path R\n",
    "    x = inp2_x\n",
    "    x = LeakyConv2D(x, f*4, (3, 1))\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    return keras.Model(ginp_x, x)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReductionA(shape, f):\n",
    "    ginp_x = layers.Input(shape)\n",
    "    x = ginp_x\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    out_x.append(x)\n",
    "    # Path 2\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*3, 1, st=2)\n",
    "    out_x.append(x)\n",
    "    # Path 3\n",
    "    x = inp_x\n",
    "    x = LeakyConv2D(x, f*2, 1)\n",
    "    x = LeakyConv2D(x, f*2, 3)\n",
    "    x = LeakyConv2D(x, f*3, 3, st=2)\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    return keras.Model(ginp_x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReductionB(shape, f):\n",
    "    ginp_x = layers.Input(shape)\n",
    "    x = ginp_x\n",
    "    inp_x = x\n",
    "    out_x = []\n",
    "    # Path 1\n",
    "    x = inp_x\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    # out_x.append(x)\n",
    "    # Path 2\n",
    "    x = inp_x\n",
    "    # x = LeakyConv2D(x, f*3, 1)\n",
    "    x = LeakyConv2D(x, f*3, 3, st=2)\n",
    "    out_x.append(x)\n",
    "    # Path 3\n",
    "    x = inp_x\n",
    "    # x = LeakyConv2D(x, f*4, 1)\n",
    "    x = LeakyConv2D(x, f*4, (1, 7))\n",
    "    x = LeakyConv2D(x, f*4, (7, 1))\n",
    "    x = LeakyConv2D(x, f*5, 3, st=2)\n",
    "    out_x.append(x)\n",
    "    # Combine\n",
    "    x = layers.Concatenate()(out_x)\n",
    "    return keras.Model(ginp_x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpSampleA(shape, f):\n",
    "    ginp_x = layers.Input(shape)\n",
    "    x = ginp_x\n",
    "    x = LeakyConv2DT(x, f*5, 3, st=2)\n",
    "    x = LeakyConv2DT(x, f*5, (7, 1))\n",
    "    x = LeakyConv2DT(x, f*5, (1, 7))\n",
    "    # x = LeakyConv2DT(x, f*4, 1)\n",
    "    return keras.Model(ginp_x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(shape):\n",
    "    ret = 1\n",
    "    for k in shape:\n",
    "        ret *= k\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    ginp_x = layers.Input((5, 512, 512))\n",
    "    x = ginp_x\n",
    "    x = layers.Reshape((5, 512, 512, 1))(x)\n",
    "    x = tf.keras.layers.ConvLSTM2D(4, 1, activation='relu', input_shape=(5,512,512,1), padding='same')(x)\n",
    "    # Encoder\n",
    "    ks = [5, 5, 5, 7, 7]\n",
    "    ef = [4, 8, 16, 32, 128]\n",
    "    for d, f in enumerate(ef):\n",
    "        x = LeakyConv2D(x, f, ks=ks[d])\n",
    "        if d == len(ef)-2:\n",
    "            tmp_x = x\n",
    "        if d != len(ef)-1:\n",
    "            x = LeakyConv2D(x, f, ks=ks[d], st=2)\n",
    "        else:\n",
    "            x = LeakyConv2D(x, f, ks=ks[d])\n",
    "    # Decoder\n",
    "    x = SEBlock(x, sq_rate=8)\n",
    "    ks = [7, 7, 5, 5, 5]\n",
    "    df = [128, 32, 16, 8, 4]\n",
    "    for d, f in enumerate(df):\n",
    "        if d == 1:\n",
    "            tmp_x = LeakyConv2D(tmp_x, 128, 3, st=2)\n",
    "            tmp_x = LeakyConv2D(tmp_x, 128, 3)\n",
    "            tmp_x = SEBlock(tmp_x, sq_rate=8)\n",
    "            x = layers.Lambda(lambda x: x[0]*x[1])([x, tmp_x])\n",
    "        if d != 0:\n",
    "            x = layers.UpSampling2D(2)(x)\n",
    "            x = LeakyConv2D(x, f, ks=ks[d])\n",
    "        else:\n",
    "            x = LeakyConv2D(x, f, ks=ks[d])\n",
    "        x = LeakyConv2D(x, f, ks=ks[d])\n",
    "    x = LeakyConv2D(x, 1, ks=3)\n",
    "    x = tf.sigmoid(x)\n",
    "    x = layers.Reshape((512, 512))(x)\n",
    "    return keras.Model(ginp_x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    ginp_x = layers.Input((512, 512))\n",
    "    x = ginp_x\n",
    "    x = layers.Reshape((512, 512, 1))(x)\n",
    "    # Encoder\n",
    "    ks = [5, 5, 5, 7, 7]\n",
    "    ef = [4, 8, 16, 32, 128]\n",
    "    for d, f in enumerate(ef):\n",
    "        x = LeakyConv2D(x, f, ks=ks[d])\n",
    "        if d == len(ef)-2:\n",
    "            tmp_x = x\n",
    "        if d != len(ef)-1:\n",
    "            x = LeakyConv2D(x, f, ks=ks[d], st=2)\n",
    "        else:\n",
    "            x = LeakyConv2D(x, f, ks=ks[d])\n",
    "    # Decoder\n",
    "    x = SEBlock(x, sq_rate=8)\n",
    "    ks = [7, 7, 5, 5, 5]\n",
    "    df = [128, 32, 16, 8, 4]\n",
    "    for d, f in enumerate(df):\n",
    "        if d == 1:\n",
    "            tmp_x = LeakyConv2D(tmp_x, 128, 3, st=2)\n",
    "            tmp_x = LeakyConv2D(tmp_x, 128, 3)\n",
    "            tmp_x = SEBlock(tmp_x, sq_rate=8)\n",
    "            x = layers.Lambda(lambda x: x[0]*x[1])([x, tmp_x])\n",
    "        if d != 0:\n",
    "            x = layers.UpSampling2D(2)(x)\n",
    "            x = LeakyConv2D(x, f, ks=ks[d])\n",
    "        else:\n",
    "            x = LeakyConv2D(x, f, ks=ks[d])\n",
    "        x = LeakyConv2D(x, f, ks=ks[d])\n",
    "    x = LeakyConv2D(x, 1, ks=3)\n",
    "    x = tf.sigmoid(x)\n",
    "    x = layers.Reshape((512, 512))(x)\n",
    "    return keras.Model(ginp_x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to use our pretrained model to reproduce our result. you can just go to the \"Uses the trained model to predict the evaluation data\", which is in the bottome of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to try to train a new model. please use the following instruction.\n",
    "If you want to train the CNN model, you need set the model = CNN in the following box. If you want to train the RNN model, you need to set the model = RNN() in the following box. \n",
    "Then just run this code untill next instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "# model = RNN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_imp(imp_image, pre_image, smooth=1e-6):\n",
    "    intersection = tf.reduce_sum(tf.abs(imp_image*pre_image))\n",
    "    return 1-(2.*intersection+smooth)/(tf.reduce_sum(tf.square(imp_image))+tf.reduce_sum(tf.square(pre_image))+smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(imp_image, def_image):\n",
    "    pre_image = model(def_image)\n",
    "    loss = tf.zeros(shape=())\n",
    "    loss = loss + dice_loss_imp(imp_image, pre_image)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(imp_image, def_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(imp_image, def_image)   \n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our method we will build three model that base on the method of the slice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code For Generate Y Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train the CNN model, please go to the \"train the CNN model\" part. If you want to train the RNN model, please go to the \"train the RNN model\" part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skulls = glob.glob(\"./CNN_dataset/implant/registrations/y_axis/*.png\")\n",
    "case = sorted([os.path.basename(s).split(\".\")[0] for s in skulls])\n",
    "print(len(case))\n",
    "t_imp_list=[]\n",
    "for i, tcase in enumerate(case):\n",
    "    t_imp_list.append(tcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "model.set_weights(weights)\n",
    "optimizer=keras.optimizers.Adam(learning_rate=.00005, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle (t_imp_list)\n",
    "sum_loss=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code in the following box for one time is one iteration. Generally, we need to run it for at least 8 times to get a better result. If you want to save the checkpoint for evary iteration, please change the filename in the filepath variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "t_list2=[]\n",
    "avgeloss = []\n",
    "error_image=0\n",
    "for j in range(len(t_imp_list)):   \n",
    "    \n",
    "    x = np.empty((1, 512, 512), dtype='float32')\n",
    "    y = np.empty((1, 512, 512), dtype='float32')\n",
    "\n",
    "    ID = t_imp_list[j]\n",
    "    try: \n",
    "        def_img = Image.open(\"./CNN_dataset/defective_skull/registrations/y_axis/\" +ID+ \".png\")\n",
    "        imp_img = Image.open(\"./CNN_dataset/implant/registrations/y_axis/\" +ID+ \".png\")\n",
    "        def_img_array = np.array(def_img)\n",
    "        imp_img_array = np.array(imp_img)\n",
    "    except:\n",
    "        print(\"error:image\", ID)\n",
    "        error_image=1\n",
    "    if error_image==1:\n",
    "        t_list2.append(ID)\n",
    "        error_image=0   \n",
    "    else:\n",
    "        i=i+1\n",
    "        x[0]=def_img_array\n",
    "        y[0]=imp_img_array\n",
    "\n",
    "        def_image = tf.convert_to_tensor(x)\n",
    "        imp_image = tf.convert_to_tensor(y)\n",
    "\n",
    "\n",
    "        loss, grads = compute_loss_and_grads(\n",
    "            imp_image, def_image\n",
    "        )\n",
    "        test = list(zip(grads, model.trainable_weights))\n",
    "\n",
    "        optimizer.apply_gradients(test)\n",
    "        avgeloss.append(loss)\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        if i % 1000 == 0:\n",
    "            aveg_loss=np.mean(avgeloss)\n",
    "            sum_loss.append(aveg_loss)\n",
    "            filepath='./modelForY/test_model_1.h5'\n",
    "            model.save_weights(filepath)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Train the RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the image list for a speical folder. please set the filepath of the skulls variable to the place where you put your nrrd file. When you changes this variable you also need to change the path for the input_image and output_image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skulls = glob.glob(\"./nrrd/implant/bilateral/*.nrrd\")\n",
    "case = sorted([os.path.basename(s).split(\".\")[0] for s in skulls])\n",
    "t_imp_list=[]\n",
    "print(len(case))\n",
    "for i, tcase in enumerate(case):\n",
    "    t_imp_list.append(tcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "model.set_weights(weights)\n",
    "optimizer=keras.optimizers.Adam(learning_rate=.00005, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code in the following box for one time is one iteration for a folder. Generally, we need to run it for at least 2 times to get a better result. For the random1 and random2, you need to first run this model on the first three folders for 2 time, then run it on the random1 and random2 folders for 2 time. If you want to save the checkpoint for evary iteration, please change the filename in the filepath variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, 100, 5):   \n",
    "    avgeloss = []\n",
    "    input_image = np.empty((10, 512, 512, 512), dtype='float32')\n",
    "    output_image = np.empty((10, 512, 512, 512), dtype='float32')\n",
    "    imp_image = np.empty((1, 512, 512), dtype='float32')\n",
    "    def_image = np.empty((1, 5, 512, 512), dtype='float32')\n",
    "    \n",
    "    for i in range(10):\n",
    "        ID = t_imp_list[j+i]\n",
    "        input_image[i], option=nrrd.read(\"./nrrd/defective_skull/bilateral/\"+ID+\".nrrd\")\n",
    "        output_image[i], option=nrrd.read(\"./nrrd/implant/bilateral/\"+ID+\".nrrd\")\n",
    "    image_range=np.arange(5120)\n",
    "    random.shuffle (image_range)\n",
    "    for i in range(len(image_range)):\n",
    "        get_image = image_range[i]\n",
    "        if get_image < 512:\n",
    "            skullNo = 0\n",
    "            imageNo = get_image\n",
    "        elif (512*1) <= get_image < (512*2):\n",
    "            skullNo = 1\n",
    "            imageNo = get_image-(512*1)\n",
    "        elif (512*2) <= get_image < (512*3):\n",
    "            skullNo = 2\n",
    "            imageNo = get_image-(512*2)\n",
    "        elif (512*3) <= get_image < (512*4):\n",
    "            skullNo = 3\n",
    "            imageNo = get_image-(512*3)\n",
    "        elif (512*4) <= get_image < (512*5):\n",
    "            skullNo = 4\n",
    "            imageNo = get_image-(512*4)\n",
    "        elif (512*5) <= get_image < (512*6):\n",
    "            skullNo = 5\n",
    "            imageNo = get_image-(512*5)\n",
    "        elif (512*6) <= get_image < (512*7):\n",
    "            skullNo = 6\n",
    "            imageNo = get_image-(512*6)\n",
    "        elif (512*7) <= get_image < (512*8):\n",
    "            skullNo = 7\n",
    "            imageNo = get_image-(512*7)\n",
    "        elif (512*8) <= get_image < (512*9):\n",
    "            skullNo = 8\n",
    "            imageNo = get_image-(512*8)\n",
    "        elif (512*9) <= get_image < (512*10):\n",
    "            skullNo = 9\n",
    "            imageNo = get_image-(512*9)\n",
    "            \n",
    "        if imageNo<2 or imageNo > 508:\n",
    "            print(skullNo)\n",
    "            print(imageNo)\n",
    "        else: \n",
    "            for k in range(5):\n",
    "                def_image[0, k]=input_image[skullNo, :, imageNo-2+k, :]\n",
    "            imp_image[0]=output_image[skullNo, :, imageNo, :]\n",
    "            \n",
    "            def_image_ten = tf.convert_to_tensor(def_image)\n",
    "            imp_image_ten = tf.convert_to_tensor(imp_image)\n",
    "    \n",
    "            loss, grads = compute_loss_and_grads(\n",
    "                imp_image_ten, def_image_ten\n",
    "            )\n",
    "            test = list(zip(grads, model.trainable_weights))\n",
    "\n",
    "            optimizer.apply_gradients(test)\n",
    "            avgeloss.append(loss)\n",
    "            print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "            if i % 1000 == 0 or i == 5119:\n",
    "                aveg_loss=np.mean(avgeloss)\n",
    "                sum_loss.append(aveg_loss)\n",
    "                filepath='./modelForY/bilateral_model_1.h5'\n",
    "                model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code For Generate Z Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train the CNN model, please go to the \"train the CNN model\" part. If you want to train the RNN model, please go to the \"train the RNN model\" part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skulls = glob.glob(\"./CNN_dataset/implant/registrations/z_axis/*.png\")\n",
    "case = sorted([os.path.basename(s).split(\".\")[0] for s in skulls])\n",
    "t_imp_list=[]\n",
    "print(len(case))\n",
    "for i, tcase in enumerate(case):\n",
    "        t_imp_list.append(tcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "model.set_weights(weights)\n",
    "optimizer=keras.optimizers.Adam(learning_rate=.00005, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle (t_imp_list)\n",
    "avgeloss = []\n",
    "sum_loss=[]\n",
    "i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code in the following box for one time is one iteration. Generally, we need to run it for at least 12 times to get a better result. If you want to save the checkpoint for evary iteration, please change the filename in the filepath variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "t_list2=[]\n",
    "avgeloss = []\n",
    "error_image=0\n",
    "for j in range(len(t_imp_list)):   \n",
    "    \n",
    "    x = np.empty((1, 512, 512), dtype='float32')\n",
    "    y = np.empty((1, 512, 512), dtype='float32')\n",
    "\n",
    "    ID = t_imp_list[j]\n",
    "    try:\n",
    "        def_img = Image.open(\"./CNN_dataset/defective_skull/registrations/z_axis/\" +ID+ \".png\")\n",
    "        imp_img = Image.open(\"./CNN_dataset/implant/registrations/z_axis/\" +ID+ \".png\")\n",
    "        def_img_array = np.array(def_img)\n",
    "        imp_img_array = np.array(imp_img)\n",
    "    \n",
    "    except:\n",
    "        print(\"error:image\", ID)\n",
    "        error_image=1\n",
    "    if error_image==1:\n",
    "        t_list2.append(ID)\n",
    "        error_image=0   \n",
    "    else:\n",
    "        i=i+1\n",
    "        x[0]=def_img_array\n",
    "        y[0]=imp_img_array\n",
    "\n",
    "        def_image = tf.convert_to_tensor(x)\n",
    "        imp_image = tf.convert_to_tensor(y)\n",
    "\n",
    "\n",
    "        loss, grads = compute_loss_and_grads(\n",
    "            imp_image, def_image\n",
    "        )\n",
    "        test = list(zip(grads, model.trainable_weights))\n",
    "\n",
    "        optimizer.apply_gradients(test)\n",
    "        avgeloss.append(loss)\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        if i % 1000 == 0:\n",
    "            aveg_loss=np.mean(avgeloss)\n",
    "            sum_loss.append(aveg_loss)\n",
    "            filepath='./modelForZ/test_model_1.h5'\n",
    "            model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Train the RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the image list for a speical folder. please set the filepath of the skulls variable to the place where you put your nrrd file. When you changes this variable you also need to change the path for the input_image and output_image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skulls = glob.glob(\"./nrrd/implant/bilateral/*.nrrd\")\n",
    "case = sorted([os.path.basename(s).split(\".\")[0] for s in skulls])\n",
    "t_imp_list=[]\n",
    "print(len(case))\n",
    "for i, tcase in enumerate(case):\n",
    "    t_imp_list.append(tcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=.00005, clipnorm=1.)\n",
    "weights = model.get_weights()\n",
    "weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code in the following box for one time is one iteration for a folder. Generally, we need to run it for at least 4 times to get a better result. For the random1 and random2, you need to first run this model on the first three folders for 2 time, then run it on the random1 and random2 folders for 5 time. If you want to save the checkpoint for evary iteration, please change the filename in the filepath variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, 100, 5):   \n",
    "    avgeloss = []\n",
    "    input_image = np.empty((10, 512, 512, 512), dtype='float32')\n",
    "    output_image = np.empty((10, 512, 512, 512), dtype='float32')\n",
    "    imp_image = np.empty((1, 512, 512), dtype='float32')\n",
    "    def_image = np.empty((1, 5, 512, 512), dtype='float32')\n",
    "    \n",
    "    for i in range(10):\n",
    "        ID = t_imp_list[j+i]\n",
    "        input_image[i], option=nrrd.read(\"./nrrd/defective_skull/bilateral/\"+ID+\".nrrd\")\n",
    "        output_image[i], option=nrrd.read(\"./nrrd/implant/bilateral/\"+ID+\".nrrd\")\n",
    "    image_range=np.arange(5120)\n",
    "    random.shuffle (image_range)\n",
    "    for i in range(len(image_range)):\n",
    "        get_image = image_range[i]\n",
    "        if get_image < 512:\n",
    "            skullNo = 0\n",
    "            imageNo = get_image\n",
    "        elif (512*1) <= get_image < (512*2):\n",
    "            skullNo = 1\n",
    "            imageNo = get_image-(512*1)\n",
    "        elif (512*2) <= get_image < (512*3):\n",
    "            skullNo = 2\n",
    "            imageNo = get_image-(512*2)\n",
    "        elif (512*3) <= get_image < (512*4):\n",
    "            skullNo = 3\n",
    "            imageNo = get_image-(512*3)\n",
    "        elif (512*4) <= get_image < (512*5):\n",
    "            skullNo = 4\n",
    "            imageNo = get_image-(512*4)\n",
    "        elif (512*5) <= get_image < (512*6):\n",
    "            skullNo = 5\n",
    "            imageNo = get_image-(512*5)\n",
    "        elif (512*6) <= get_image < (512*7):\n",
    "            skullNo = 6\n",
    "            imageNo = get_image-(512*6)\n",
    "        elif (512*7) <= get_image < (512*8):\n",
    "            skullNo = 7\n",
    "            imageNo = get_image-(512*7)\n",
    "        elif (512*8) <= get_image < (512*9):\n",
    "            skullNo = 8\n",
    "            imageNo = get_image-(512*8)\n",
    "        elif (512*9) <= get_image < (512*10):\n",
    "            skullNo = 9\n",
    "            imageNo = get_image-(512*9)\n",
    "            \n",
    "        if imageNo<2 or imageNo > 508:\n",
    "            print(skullNo)\n",
    "            print(imageNo)\n",
    "        else: \n",
    "            for k in range(5):\n",
    "                def_image[0, k]=input_image[skullNo, :, :, imageNo-2+k]\n",
    "            imp_image[0]=output_image[skullNo, :, :, imageNo]\n",
    "            \n",
    "            def_image_ten = tf.convert_to_tensor(def_image)\n",
    "            imp_image_ten = tf.convert_to_tensor(imp_image)\n",
    "    \n",
    "            loss, grads = compute_loss_and_grads(\n",
    "                imp_image_ten, def_image_ten\n",
    "            )\n",
    "            test = list(zip(grads, model.trainable_weights))\n",
    "\n",
    "            optimizer.apply_gradients(test)\n",
    "            avgeloss.append(loss)\n",
    "            print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "            if i % 1000 == 0 or i == 5119:\n",
    "                aveg_loss=np.mean(avgeloss)\n",
    "                sum_loss.append(aveg_loss)\n",
    "                filepath='./modelForZ/bilateral_test_model_1.h5'\n",
    "                model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Generate X Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train the CNN model, please go to the \"train the CNN model\" part. If you want to train the RNN model, please go to the \"train the RNN model\" part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skulls = glob.glob(\"./CNN_dataset/implant/registrations/x_axis/*.png\")\n",
    "case = sorted([os.path.basename(s).split(\".\")[0] for s in skulls])\n",
    "t_imp_list=[]\n",
    "print(len(case))\n",
    "for i, tcase in enumerate(case):\n",
    "        t_imp_list.append(tcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "model.set_weights(weights)\n",
    "optimizer=keras.optimizers.Adam(learning_rate=.00005, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle (t_imp_list)\n",
    "t_list2=[]\n",
    "avgeloss = []\n",
    "sum_loss=[]\n",
    "i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code in the following box for one time is one iteration. Generally, we need to run it for at least 13 times to get a better result. If you want to save the checkpoint for evary iteration, please change the filename in the filepath variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_image=0\n",
    "for j in range(len(t_imp_list)):   \n",
    "    \n",
    "    x = np.empty((1, 512, 512), dtype='float32')\n",
    "    y = np.empty((1, 512, 512), dtype='float32')\n",
    "    ID = t_imp_list[j]\n",
    "    try: \n",
    "        def_img = Image.open(\"./CNN_dataset/defective_skull/registrations/x_axis/\" +ID+ \".png\")\n",
    "        imp_img = Image.open(\"./CNN_dataset/implant/registrations/x_axis/\" +ID+ \".png\")\n",
    "        def_img_array = np.array(def_img)\n",
    "        imp_img_array = np.array(imp_img)\n",
    "    except:\n",
    "        print(\"error:image\", ID)\n",
    "        error_image=1\n",
    "    if error_image==1:\n",
    "        t_list2.append(ID)\n",
    "        error_image=0   \n",
    "    else:\n",
    "        i=i+1\n",
    "        x[0]=def_img_array\n",
    "        y[0]=imp_img_array\n",
    "\n",
    "        def_image = tf.convert_to_tensor(x)\n",
    "        imp_image = tf.convert_to_tensor(y)\n",
    "\n",
    "        loss, grads = compute_loss_and_grads(\n",
    "            imp_image, def_image\n",
    "        )\n",
    "        test = list(zip(grads, model.trainable_weights))\n",
    "\n",
    "        optimizer.apply_gradients(test)\n",
    "        avgeloss.append(loss)\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        if i % 1000 == 0:\n",
    "            aveg_loss=np.mean(avgeloss)\n",
    "            sum_loss.append(aveg_loss)\n",
    "            filepath='./modelForX/test_model_1.h5'\n",
    "            model.save_weights(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Train the RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the image list for a speical folder. please set the filepath of the skulls variable to the place where you put your nrrd file. When you changes this variable you also need to change the path for the input_image and output_image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skulls = glob.glob(\"./nrrd/implant/bilateral/*.nrrd\")\n",
    "case = sorted([os.path.basename(s).split(\".\")[0] for s in skulls])\n",
    "t_imp_list=[]\n",
    "print(len(case))\n",
    "for i, tcase in enumerate(case):\n",
    "    t_imp_list.append(tcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=.00005, clipnorm=1.)\n",
    "weights = model.get_weights()\n",
    "weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code in the following box for one time is one iteration for a folder. Generally, we need to run it for at least 2 times to get a better result. For the random1 and random2, you need to first run this model on the first three folders for 2 time, then run it on the random1 and random2 folders for 2 time. If you want to save the checkpoint for evary iteration, please change the filename in the filepath variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, 100, 5):   \n",
    "    avgeloss = []\n",
    "    input_image = np.empty((10, 512, 512, 512), dtype='float32')\n",
    "    output_image = np.empty((10, 512, 512, 512), dtype='float32')\n",
    "    imp_image = np.empty((1, 512, 512), dtype='float32')\n",
    "    def_image = np.empty((1, 5, 512, 512), dtype='float32')\n",
    "    \n",
    "    for i in range(10):\n",
    "        ID = t_imp_list[j+i]\n",
    "        input_image[i], option=nrrd.read(\"./nrrd/defective_skull/bilateral/\"+ID+\".nrrd\")\n",
    "        output_image[i], option=nrrd.read(\"./nrrd/implant/bilateral/\"+ID+\".nrrd\")\n",
    "    image_range=np.arange(5120)\n",
    "    random.shuffle (image_range)\n",
    "    for i in range(len(image_range)):\n",
    "        get_image = image_range[i]\n",
    "        if get_image < 512:\n",
    "            skullNo = 0\n",
    "            imageNo = get_image\n",
    "        elif (512*1) <= get_image < (512*2):\n",
    "            skullNo = 1\n",
    "            imageNo = get_image-(512*1)\n",
    "        elif (512*2) <= get_image < (512*3):\n",
    "            skullNo = 2\n",
    "            imageNo = get_image-(512*2)\n",
    "        elif (512*3) <= get_image < (512*4):\n",
    "            skullNo = 3\n",
    "            imageNo = get_image-(512*3)\n",
    "        elif (512*4) <= get_image < (512*5):\n",
    "            skullNo = 4\n",
    "            imageNo = get_image-(512*4)\n",
    "        elif (512*5) <= get_image < (512*6):\n",
    "            skullNo = 5\n",
    "            imageNo = get_image-(512*5)\n",
    "        elif (512*6) <= get_image < (512*7):\n",
    "            skullNo = 6\n",
    "            imageNo = get_image-(512*6)\n",
    "        elif (512*7) <= get_image < (512*8):\n",
    "            skullNo = 7\n",
    "            imageNo = get_image-(512*7)\n",
    "        elif (512*8) <= get_image < (512*9):\n",
    "            skullNo = 8\n",
    "            imageNo = get_image-(512*8)\n",
    "        elif (512*9) <= get_image < (512*10):\n",
    "            skullNo = 9\n",
    "            imageNo = get_image-(512*9)\n",
    "            \n",
    "        if imageNo<2 or imageNo > 508:\n",
    "            print(skullNo)\n",
    "            print(imageNo)\n",
    "        else: \n",
    "            for k in range(5):\n",
    "                def_image[0, k]=input_image[skullNo, imageNo-2+k, :, :]\n",
    "            imp_image[0]=output_image[skullNo, imageNo, :, :]\n",
    "            \n",
    "            def_image_ten = tf.convert_to_tensor(def_image)\n",
    "            imp_image_ten = tf.convert_to_tensor(imp_image)\n",
    "    \n",
    "            loss, grads = compute_loss_and_grads(\n",
    "                imp_image_ten, def_image_ten\n",
    "            )\n",
    "            test = list(zip(grads, model.trainable_weights))\n",
    "\n",
    "            optimizer.apply_gradients(test)\n",
    "            avgeloss.append(loss)\n",
    "            print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "            if i % 1000 == 0 or i == 5119:\n",
    "                aveg_loss=np.mean(avgeloss)\n",
    "                sum_loss.append(aveg_loss)\n",
    "                filepath='./modelForX/bilateral_model_1.h5'\n",
    "                model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses the trained model to predict the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_x_m = RNN()\n",
    "d2_y_m = RNN()\n",
    "d2_z_m = RNN()\n",
    "d2_x_m1 = CNN()\n",
    "d2_y_m1 = CNN()\n",
    "d2_z_m1 = CNN()\n",
    "\n",
    "d2_x_m.load_weights('./PreTrainedModel/modelForX/bilateral_model_2.h5')\n",
    "d2_y_m.load_weights('./PreTrainedModel/modelForY/bilateral_model_1.h5')\n",
    "d2_z_m.load_weights('./PreTrainedModel/modelForZ/bilateral_model_4.h5')\n",
    "\n",
    "d2_x_m1.load_weights('./PreTrainedModel/modelForX/CNN_model_13.h5')\n",
    "d2_y_m1.load_weights('./PreTrainedModel/modelForY/CNN_model_8.h5')\n",
    "d2_z_m1.load_weights('./PreTrainedModel/modelForZ/CNN_model_12.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please change the prefix varible to the place that you your nrrd that want to be evalated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(ID):\n",
    "    prefix = './eval_data/defective_skull/bilateral/'\n",
    "    def_data, info = nrrd.read(os.path.join(prefix, f'{ID:03d}.nrrd'))\n",
    "    return def_data, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_z(def_data):\n",
    "    ret = np.zeros((512, 512, 512), dtype='float32')\n",
    "    for k in range(512):\n",
    "        if k<2 or k>508:\n",
    "            print(\"****\")\n",
    "        else:\n",
    "            def_slc = np.zeros((1, 5, 512, 512), dtype='float32')\n",
    "            for i in range(5):\n",
    "                def_slc[0, i]=def_data[:, :, k-2+i]\n",
    "            imp_slc = d2_z_m.predict(def_slc)\n",
    "            ret[:,:,k] = imp_slc\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x(def_data):\n",
    "    ret = np.zeros((512, 512, 512), dtype='float32')\n",
    "    for k in range(512):\n",
    "        if k<2 or k>508:\n",
    "            print(\"****\")\n",
    "        else:\n",
    "            def_slc = np.zeros((1, 5, 512, 512), dtype='float32')\n",
    "            for i in range(5):\n",
    "                def_slc[0, i]=def_data[k-2+i]\n",
    "            imp_slc = d2_x_m.predict(def_slc)\n",
    "            ret[k] = imp_slc\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(def_data):\n",
    "    ret = np.zeros((512, 512, 512), dtype='float32')\n",
    "    for k in range(512):\n",
    "        if k<2 or k>508:\n",
    "            print(\"****\")\n",
    "        else:\n",
    "            def_slc = np.zeros((1, 5, 512, 512), dtype='float32')\n",
    "            for i in range(5):\n",
    "                def_slc[0, i]=def_data[:,k-2+i,:]\n",
    "            imp_slc = d2_y_m.predict(def_slc)\n",
    "            ret[:,k,:] = imp_slc\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x1(def_data):\n",
    "    ret = np.zeros((512, 512, 512), dtype='float32')\n",
    "    for k in range(512):\n",
    "        def_slc = np.zeros((1, 512, 512), dtype='float32')\n",
    "        def_slc[0]=def_data[k]\n",
    "        if def_slc[0].sum()==0:\n",
    "            ret[k] = def_slc[0]\n",
    "        else:\n",
    "            imp_slc = d2_x_m1.predict(def_slc)\n",
    "            ret[k] = imp_slc\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y1(def_data):\n",
    "    ret = np.zeros((512, 512, 512), dtype='float32')\n",
    "    for k in range(512):\n",
    "        def_slc = np.zeros((1, 512, 512), dtype='float32')\n",
    "        def_slc[0]=def_data[:, k, :]\n",
    "        if def_slc[0].sum()==0:\n",
    "            ret[:, k, :] = def_slc[0]\n",
    "        else:\n",
    "            imp_slc = d2_y_m1.predict(def_slc)\n",
    "            ret[:, k, :] = imp_slc\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_z1(def_data):\n",
    "    ret = np.zeros((512, 512, 512), dtype='float32')\n",
    "    for k in range(512):\n",
    "        def_slc = np.zeros((1, 512, 512), dtype='float32')\n",
    "        def_slc[0]=def_data[:, :, k]\n",
    "        if def_slc[0].sum()==0:\n",
    "            ret[:, :, k] = def_slc[0]\n",
    "        else:\n",
    "            imp_slc = d2_z_m1.predict(def_slc)\n",
    "            ret[:, :, k] = imp_slc\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the nrrd.write line, you can set the place where you want to put your evalated nrrd file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for image in range(20):\n",
    "    def_data, info = get_test_data(image)\n",
    "    \n",
    "    retx = predict_x(def_data)\n",
    "    rety = predict_y(def_data)\n",
    "    retz = predict_z(def_data)\n",
    "    \n",
    "    retx1 = predict_x1(def_data)\n",
    "    rety1 = predict_y1(def_data)\n",
    "    retz1 = predict_z1(def_data)\n",
    "    \n",
    "    Ret=retx+rety+retz+retx1+rety1+retz1\n",
    "    ret_save1=np.empty((512, 512, 512), dtype='int32')\n",
    "    ret_save2=np.empty((512, 512, 512), dtype='int32')\n",
    "\n",
    "    \n",
    "    for i in range(Ret.shape[0]):\n",
    "        for j in range(Ret.shape[1]):\n",
    "            for k in range(Ret.shape[2]):\n",
    "                if (Ret[i, j, k] > 1.0):\n",
    "                      ret_save1[i,j,k]=1\n",
    "    \n",
    "    \n",
    "    for i in range(Ret.shape[0]):\n",
    "        for j in range(Ret.shape[1]):\n",
    "            for k in range(Ret.shape[2]):\n",
    "                if (ret_save1[i, j, k] == 1):\n",
    "#                   remove the small impurity\n",
    "                    round_el1 = np.empty((7, 7, 7))\n",
    "                    round_el1 = ret_save1[i-3:i+4, j-3:j+4, k-3:k+4]\n",
    "#                   remove the large impurity\n",
    "                    round_el2 = np.empty((11, 11, 11))\n",
    "#                   due to the outline of the large impurity may beyond the 512,\n",
    "#                   So do this process\n",
    "                    if (i+7>512 or j+7>512 or k+7>512):\n",
    "                        round_el2 = ret_save1[i-11:i, j-11:j, k-11:k]\n",
    "                    else: \n",
    "                        round_el2 = ret_save1[i-5:i+6, j-5:j+6, k-5:k+6]\n",
    "                        \n",
    "                    if (round_el2.sum()>=500 and round_el1.sum()>=90):\n",
    "                        ret_save2[i, j, k] = 1   \n",
    "                    \n",
    "               \n",
    "    nrrd.write(os.path.join('./result/bilateral', f'{image:03d}.nrrd'), ret_save2.astype('int32'), info)\n",
    "    print(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
